{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78d5361c-663a-4198-b847-d480087261f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4b2c03d3e6c50da2e3a8558dbee3683",
     "grade": false,
     "grade_id": "cell-087b71a01dc13ac4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2024.11.01 - Generative AI |Â Homework Assignment 2\n",
    "\n",
    "In this exercise, you will implement the food-for-thought-generating prompts strategy presented by Prof. Gottlob in his lecture. In the food-for-thought-generating prompt strategy we want, we first want to ask an LLM to provide questions that help solve the original query.\n",
    "\n",
    "\n",
    "*Relevant-Slide:*\n",
    "<img src=\"./food-for-thought.png\"/>\n",
    "\n",
    "Passages where you should add your implementation are marked with:\n",
    "\n",
    "\\# YOUR CODE HERE</br>\n",
    "raise NotImplementedError()\n",
    "\n",
    "We have provided a simple Hugging Face wrapper so you can test your implementation against an actual llm. To do so, you will need to provide your [personal access token](https://huggingface.co/docs/hub/security-tokens).\n",
    "The grading is based on a mocked client, similar to visible test cases. Therefore, when you are confident with your implementation, delete your personal access token before submitting the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b588a32-7e10-4a65-b2ae-55afe423772b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7579a07d4107991067c1f4e00ce92aa8",
     "grade": false,
     "grade_id": "cell-7d0507cf18a6ba54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import requests\n",
    "import re\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7541db0c-f3cc-4ba7-a436-03afbf34dc1c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "480bff9c287696915e034fccdc611673",
     "grade": false,
     "grade_id": "cell-b081763e2fde4c03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Message:\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class LLMClientInterface(ABC):\n",
    "    @abstractmethod\n",
    "    def get_completion(self, messages: List[Message]) -> str:\n",
    "        pass\n",
    "\n",
    "class HuggingFaceClient(LLMClientInterface):\n",
    "    def __init__(self, api_token: str, model_name: str = \"HuggingFaceH4/zephyr-7b-alpha\"):\n",
    "        self.api_token = api_token\n",
    "        self.model_name = model_name\n",
    "        self.api_url = f\"https://api-inference.huggingface.co/models/{model_name}/v1/chat/completions\"\n",
    "        self.headers = { \"Authorization\": f\"Bearer {api_token}\", \"Content-Type\": \"application/json\" }\n",
    "\n",
    "    def get_completion(self, messages: List[Message]) -> str:\n",
    "        messages_dict = [\n",
    "            {\"role\": message.role, \"content\": message.content} for message in messages\n",
    "        ]\n",
    "        payload = { \"model\": self.model_name, \"messages\": messages_dict, \"max_tokens\": 500 }\n",
    "        response = requests.post(self.api_url, headers=self.headers, json=payload)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"API request failed with status {response.status_code}: {response.text}\")\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7c8f56-dcf5-4f77-b797-129c81a2e6a6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20ca71cb26ccdc527ab146a77956cdbc",
     "grade": false,
     "grade_id": "cell-ccfc7bfa58208b96",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "revert": "class FoodForThoughtPrompting:\n    \"\"\"\n    Implements the food-for-thought prompting strategy for enhanced LLM interactions.\n    This strategy breaks down a complex query into sub-questions, gets their answers,\n    and synthesizes a final response using this additional context.\n    \"\"\"\n    def __init__(self, llm_client: LLMClientInterface):\n        self.llm_client = llm_client\n        self.system_prompt = \"You strive to answer the question as truthfully, precisely and concisely as possible.\"\n    \n    def generate_questions(self, query: str) -> List[str]:\n        \"\"\"\n        Generates three relevant sub-questions that help break down the main query.\n        \n        Implementation Guidelines:\n        1. Create a prompt that asks the LLM to generate 3 questions whose answers would help solve \n           the original query.\n        \n        2. Get the LLM's response using self.llm_client.get_completion(messages), where messages is a list of a user prompt and maybe a system prompt (optional).\n            e.g. messages = [\n                Message(role=\"system\", content=\"Answer truthfully and concisely [...]\"),\n                Message(role=\"user\", content=\"Which are the three questions Q1, [...]\")\n            ]\n\n        \n        4. Extract the three questions from the response (e.g. using regex or simple string splitting).\n           You might need to modify your user prompt if you don't get consistently formatted completions.\n        \n        Args:\n            query (str): The original query to be broken down\n            \n        Returns:\n            List[str]: List of exactly three questions\n        \"\"\"\n        # YOUR CODE HERE\n        raise NotImplementedError()\n    \n    def get_answers(self, questions: List[str]) -> List[str]:\n        \"\"\"\n        Generate answers for each of the generated sub-questions.\n\n        Args:\n            questions (List[str]): List of questions to be answered\n            \n        Returns:\n            List[str]: List of answers corresponding to each question generated by the LLM.\n        \"\"\"\n        # YOUR CODE HERE\n        raise NotImplementedError()\n    \n    def get_final_answer(self, query: str, qa_pairs: List[Tuple[str, str]]) -> str:\n        \"\"\"\n        Synthesizes a final answer using the original query and Q&A context.\n        \n        Implementation Guidelines:\n        1. Create a messages list optionally starting with the system prompt Message.\n           It might help to tell the LLM that the whole conversation should be incorporated into its final response\n           in the system prompt.\n        \n        2. For each (question, answer) pair in qa_pairs:\n           - Add two Message objects:\n             * First with role=\"user\" containing the question\n             * Second with role=\"assistant\" containing the answer\n        \n        3. Add a final Message with role=\"user\" containing the original query\n        \n        4. Get and return the final response using self.llm_client.get_completion()\n        \n        Args:\n            query (str): The original query\n            qa_pairs (List[Tuple[str, str]]): List of (question, answer) tuples providing context\n            \n        Returns:\n            str: Synthesized final answer incorporating the context\n        \"\"\"\n        # YOUR CODE HERE\n        raise NotImplementedError()\n    \n    def __call__(self, query: str) -> str:\n        try:\n            questions = self.generate_questions(query)            \n            answers = self.get_answers(questions)            \n            qa_pairs = list(zip(questions, answers))            \n            final_answer = self.get_final_answer(query, qa_pairs)\n            return final_answer\n        except Exception as e:\n            return f\"Error processing query: {str(e)}\""
   },
   "outputs": [],
   "source": [
    "class FoodForThoughtPrompting:\n",
    "    \"\"\"\n",
    "    Implements the food-for-thought prompting strategy for enhanced LLM interactions.\n",
    "    This strategy breaks down a complex query into sub-questions, gets their answers,\n",
    "    and synthesizes a final response using this additional context.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm_client: LLMClientInterface):\n",
    "        self.llm_client = llm_client\n",
    "        self.system_prompt = \"You strive to answer the question as truthfully, precisely and concisely as possible.\"\n",
    "    \n",
    "    def generate_questions(self, query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generates three relevant sub-questions that help break down the main query.\n",
    "        \n",
    "        Implementation Guidelines:\n",
    "        1. Create a prompt that asks the LLM to generate 3 questions whose answers would help solve \n",
    "           the original query.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        init_query = f\"Which are the three questions Q1,Q2 and Q3 whose answers would be most helpful to solve the following problem: {query}\"\n",
    "\n",
    "        \"\"\"\n",
    "        2. Get the LLM's response using self.llm_client.get_completion(messages), where messages is a list of a user prompt and maybe a system prompt (optional).\n",
    "            e.g. messages = [\n",
    "                Message(role=\"system\", content=\"Answer truthfully and concisely [...]\"),\n",
    "                Message(role=\"user\", content=\"Which are the three questions Q1, [...]\")\n",
    "            ]\n",
    "\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "        Message(role=\"system\", content=\"You are a helpful assistant. Please provide three clear and concise questions labeled as Q1, Q2, and Q3.\"),\n",
    "        Message(role=\"user\", content=init_query)\n",
    "        ]\n",
    "\n",
    "        response = self.llm_client.get_completion(messages)\n",
    "        \"\"\"\n",
    "\n",
    "        4. Extract the three questions from the response (e.g. using regex or simple string splitting).\n",
    "           You might need to modify your user prompt if you don't get consistently formatted completions.\n",
    "        \n",
    "        Args:\n",
    "            query (str): The original query to be broken down\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: List of exactly three questions\n",
    "        \"\"\"\n",
    "\n",
    "        def _extract_questions(repsponse): \n",
    "            pattern = r'Q(\\d+):\\s*(.*?(?=Q\\d+:|$))'\n",
    "            matches = re.findall(pattern, response, re.DOTALL)\n",
    "            questions = [match[1].strip() for match in matches]\n",
    "            return questions[:3]  # Ensure we return exactly 3 questions\n",
    "\n",
    "        \n",
    "        questions = _extract_questions(response)\n",
    "    \n",
    "        # If we didn't get exactly 3 questions, we might want to try again or handle the error\n",
    "        if len(questions) != 3:\n",
    "            raise ValueError(f\"Expected 3 questions, but got {len(questions)}\")\n",
    "        \n",
    "        return questions\n",
    "\n",
    "    def get_answers(self, questions: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate answers for each of the generated sub-questions.\n",
    "\n",
    "        Args:\n",
    "            questions (List[str]): List of questions to be answered\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: List of answers corresponding to each question generated by the LLM.\n",
    "        \"\"\"\n",
    "        answers = []\n",
    "\n",
    "        for question in questions:\n",
    "            # Create a message for each question\n",
    "            messages = [\n",
    "                Message(role=\"system\", content=\"You are a helpful assistant. Please provide a clear and concise answer to the following question.\"),\n",
    "                Message(role=\"user\", content=question)\n",
    "            ]\n",
    "\n",
    "        # Get the LLM's response\n",
    "            response = self.llm_client.get_completion(messages)\n",
    "\n",
    "        # Add the response to our list of answers\n",
    "        answers.append(response.strip())\n",
    "\n",
    "        return answers\n",
    "    \n",
    "    def get_final_answer(self, query: str, qa_pairs: List[Tuple[str, str]]) -> str:\n",
    "        \"\"\"\n",
    "        Synthesizes a final answer using the original query and Q&A context.\n",
    "        \n",
    "        Implementation Guidelines:\n",
    "        1. Create a messages list optionally starting with the system prompt Message.\n",
    "           It might help to tell the LLM that the whole conversation should be incorporated into its final response\n",
    "           in the system prompt.\n",
    "        \n",
    "        2. For each (question, answer) pair in qa_pairs:\n",
    "           - Add two Message objects:\n",
    "             * First with role=\"user\" containing the question\n",
    "             * Second with role=\"assistant\" containing the answer\n",
    "        \n",
    "        3. Add a final Message with role=\"user\" containing the original query\n",
    "        \n",
    "        4. Get and return the final response using self.llm_client.get_completion()\n",
    "        \n",
    "        Args:\n",
    "            query (str): The original query\n",
    "            qa_pairs (List[Tuple[str, str]]): List of (question, answer) tuples providing context\n",
    "            \n",
    "        Returns:\n",
    "            str: Synthesized final answer incorporating the context\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            Message(role=\"system\", content=\"You are a helpful assistant. Your task is to provide a comprehensive answer to the original query based on the context provided in the following Q&A pairs. Incorporate all relevant information from the Q&A pairs into your final response.\")\n",
    "        ]    \n",
    "\n",
    "        for question, answer in qa_pairs:\n",
    "            messages.extend([\n",
    "            Message(role=\"user\", content=question),\n",
    "            Message(role=\"assistant\", content=answer)\n",
    "        ])\n",
    "\n",
    "        messages.append(Message(role=\"user\", content=f\"Now, based on all the information provided above, please answer this original query: {query}\"))\n",
    "\n",
    "        # Get the final response from the LLM\n",
    "        final_answer = self.llm_client.get_completion(messages)\n",
    "\n",
    "        return final_answer.strip()\n",
    "        \n",
    "    def __call__(self, query: str) -> str:\n",
    "        try:\n",
    "            questions = self.generate_questions(query)            \n",
    "            answers = self.get_answers(questions)            \n",
    "            qa_pairs = list(zip(questions, answers))            \n",
    "            final_answer = self.get_final_answer(query, qa_pairs)\n",
    "            return final_answer\n",
    "        except Exception as e:\n",
    "            return f\"Error processing query: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5d4c7ad-cd40-4b26-aea1-820b7192f52b",
   "metadata": {
    "revert": "# To test your implementation against a real LLM, provide your hugging face access token below.\n# Before submitting it, please remove your token again. Grading will be based on a mocked implementation\n# If the response seems cut off - no worries; its a known issue\n# https://huggingface.co/HuggingFaceH4/zephyr-7b-beta/discussions/52\nhuggingface_client = HuggingFaceClient(\"YOUR_HUGGING_FACE_TOKEN_HERE\")\nfft_prompting = FoodForThoughtPrompting(huggingface_client)\nfft_prompting(\"How similar are a pen and a marker, on a score ranging from 1 (lowest) to 10 (highest)?\")"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on their physical design and structure, pens and markers have many similarities but there are also notable differences. On a scale of 1 to 10, with 1 being the least similar and 10 being the most similar, pens and markers could be rated as a 7 in terms of similarity.\\n\\nBoth pens and markers are cylindrical or hexagonal in shape, have a dispensing mechanism, and come in a variety of barrel colors. However, markers typically have a wider barrel, a larger grip, and often feature a removable cap or lid, which isn't necessarily the case for pens.\\n\\nAdditionally, markers typically have chisel or bullet-shaped tips, while pens can have fine-point or broad tips. Pens do not have ink that can be refilled, whereas markers often do.\\n\\nSo while pens and markers share some similarities, they differ in terms of their intended use, design elements, and functional components, bringing their similarity score to around 7.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To test your implementation against a real LLM, provide your hugging face access token below.\n",
    "# Before submitting it, please remove your token again. Grading will be based on a mocked implementation\n",
    "# If the response seems cut off - no worries; its a known issue\n",
    "# https://huggingface.co/HuggingFaceH4/zephyr-7b-beta/discussions/52\n",
    "huggingface_client = HuggingFaceClient(\"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\")\n",
    "fft_prompting = FoodForThoughtPrompting(huggingface_client)\n",
    "fft_prompting(\"How similar are a pen and a marker, on a score ranging from 1 (lowest) to 10 (highest)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f82236cc-5111-436d-b3d0-e3778213369b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You should consider a balanced meal with protein and vegetables.']\n"
     ]
    }
   ],
   "source": [
    "mock_responses = [\n",
    "        \"You should consider a balanced meal with protein and vegetables.\",\n",
    "        \"Lunch is typically eaten between 12:00 and 2:00 PM.\",\n",
    "        \"A reasonable budget is $10-15 for a healthy lunch.\"\n",
    "    ]\n",
    "    \n",
    "client = MockLLMClient(mock_responses)\n",
    "fft = FoodForThoughtPrompting(client)\n",
    "    \n",
    "questions = [\n",
    "        \"What makes a healthy lunch?\",\n",
    "        \"When should I eat lunch?\",\n",
    "        \"How much should I spend on lunch?\"\n",
    "]\n",
    "    \n",
    "answers = fft.get_answers(questions)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d41676f-9ad7-4fab-85b7-a30a9fae75fe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8c4c08b744df2dec345b041c5b5fb20",
     "grade": true,
     "grade_id": "cell-e7117f7da4e0dd04",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 82\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing query\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result\n\u001b[1;32m     81\u001b[0m test_generate_questions_success()\n\u001b[0;32m---> 82\u001b[0m \u001b[43mtest_get_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m test_process_query_end_to_end()\n\u001b[1;32m     84\u001b[0m test_error_handling()\n",
      "Cell \u001b[0;32mIn[10], line 45\u001b[0m, in \u001b[0;36mtest_get_answers\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m questions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat makes a healthy lunch?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen should I eat lunch?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow much should I spend on lunch?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m ]\n\u001b[1;32m     44\u001b[0m answers \u001b[38;5;241m=\u001b[39m fft\u001b[38;5;241m.\u001b[39mget_answers(questions)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(answers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m answers)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced meal\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m answers[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class MockLLMClient(LLMClientInterface):\n",
    "    def __init__(self, responses):\n",
    "        self.responses = responses\n",
    "        self.call_count = 0\n",
    "        self.messages_history = []\n",
    "    \n",
    "    def get_completion(self, messages: List[Message]) -> str:\n",
    "        self.messages_history.append(messages)\n",
    "        response = self.responses[self.call_count]\n",
    "        self.call_count += 1\n",
    "        return response\n",
    "\n",
    "def test_generate_questions_success():\n",
    "    mock_response = \"\"\"Q1: What are your dietary restrictions or preferences?\n",
    "Q2: What time of day will you be eating lunch?\n",
    "Q3: What is your budget for lunch?\"\"\"\n",
    "    \n",
    "    client = MockLLMClient([mock_response])\n",
    "    fft = FoodForThoughtPrompting(client)\n",
    "    questions = fft.generate_questions(\"What is the best lunch to eat?\")\n",
    "    \n",
    "    assert len(questions) == 3\n",
    "    assert all(isinstance(q, str) for q in questions)\n",
    "    assert \"dietary restrictions\" in questions[0]\n",
    "    assert \"time of day\" in questions[1]\n",
    "    assert \"budget\" in questions[2]\n",
    "\n",
    "def test_get_answers():\n",
    "    mock_responses = [\n",
    "        \"You should consider a balanced meal with protein and vegetables.\",\n",
    "        \"Lunch is typically eaten between 12:00 and 2:00 PM.\",\n",
    "        \"A reasonable budget is $10-15 for a healthy lunch.\"\n",
    "    ]\n",
    "    \n",
    "    client = MockLLMClient(mock_responses)\n",
    "    fft = FoodForThoughtPrompting(client)\n",
    "    \n",
    "    questions = [\n",
    "        \"What makes a healthy lunch?\",\n",
    "        \"When should I eat lunch?\",\n",
    "        \"How much should I spend on lunch?\"\n",
    "    ]\n",
    "    \n",
    "    answers = fft.get_answers(questions)\n",
    "    assert len(answers) == 3\n",
    "    assert all(isinstance(a, str) for a in answers)\n",
    "    assert \"balanced meal\" in answers[0]\n",
    "    assert \"12:00\" in answers[1]\n",
    "    assert \"$10-15\" in answers[2]\n",
    "\n",
    "def test_process_query_end_to_end():\n",
    "    mock_responses = [\n",
    "        # Questions generation response\n",
    "        \"\"\"Q1: What are your dietary restrictions?\n",
    "Q2: What time of day will you eat?\n",
    "Q3: What is your budget?\"\"\",\n",
    "        # Three answers\n",
    "        \"No dietary restrictions.\",\n",
    "        \"Lunchtime at 12:30 PM.\",\n",
    "        \"Budget is $15.\",\n",
    "        # Final answer\n",
    "        \"Based on the information provided, I recommend a balanced meal...\"\n",
    "    ]\n",
    "    \n",
    "    client = MockLLMClient(mock_responses)\n",
    "    fft = FoodForThoughtPrompting(client)\n",
    "    \n",
    "    result = fft(\"What should I eat for lunch?\")\n",
    "    assert isinstance(result, str)\n",
    "    assert \"balanced meal\" in result\n",
    "    assert client.call_count == 5  # 1 for questions + 3 for answers + 1 for final\n",
    "\n",
    "def test_error_handling():\n",
    "    client = MockLLMClient([])  # Empty responses will cause index error\n",
    "    fft = FoodForThoughtPrompting(client)\n",
    "    \n",
    "    result = fft(\"What should I eat?\")\n",
    "    assert \"Error processing query\" in result\n",
    "\n",
    "\n",
    "test_generate_questions_success()\n",
    "test_get_answers()\n",
    "test_process_query_end_to_end()\n",
    "test_error_handling()\n",
    "\n",
    "print(\"If you see this message, you are good to go â\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
